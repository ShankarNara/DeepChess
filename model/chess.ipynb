{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "550e0b89-f5da-4815-b536-81835cd9e6ae",
    "_uuid": "d2ea89e5-0263-420c-bd5e-24f06bca89ff"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Move.from_uci('b2b4')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "board = chess.Board() \n",
    "move = random.choice(list(board.legal_moves))\n",
    "move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_player(board):\n",
    "    move = random.choice(list(board.legal_moves))\n",
    "    return move.uci()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows us to play if we give the right uci input\n",
    "def human_player(board):\n",
    "    st = input()\n",
    "    move = chess.Move.from_uci(st)\n",
    "    return move.uci()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who(player):\n",
    "    return \"White\" if player == chess.WHITE else \"Black\"\n",
    "def display_board(board, use_svg):\n",
    "    if use_svg:\n",
    "        return board._repr_svg_()\n",
    "    else:\n",
    "        return \"<pre>\" + str(board) + \"</pre>\"\n",
    "    \n",
    "def play_game(player1, player2, visual=\"svg\", pause=0.1):\n",
    "    \"\"\"\n",
    "    playerN1, player2: functions that takes board, return uci move\n",
    "    visual: \"simple\" | \"svg\" | None\n",
    "    \"\"\"\n",
    "    use_svg = (visual == \"svg\")\n",
    "    board = chess.Board()\n",
    "    board_stop = display_board(board, use_svg)\n",
    "    html = \"%s\" % (board_stop)\n",
    "    display(HTML(html))\n",
    "    try:\n",
    "        while not board.is_game_over(claim_draw=True):\n",
    "            if board.turn == chess.WHITE:\n",
    "                uci = player1(board)\n",
    "            else:\n",
    "                uci = player2(board)\n",
    "            name = who(board.turn)\n",
    "            board.push_uci(uci)\n",
    "            board_stop = display_board(board, use_svg)\n",
    "            html = \"<h1>Move %s %s, Play '%s':</h1><br/>%s\" % (\n",
    "                       len(board.move_stack), name, uci, board_stop)\n",
    "            if visual is not None:\n",
    "                if visual == \"svg\":\n",
    "                    clear_output(wait=True)\n",
    "                display(HTML(html))\n",
    "                if visual == \"svg\":\n",
    "                    time.sleep(pause)\n",
    "    except KeyboardInterrupt:\n",
    "        msg = \"Game interrupted!\"\n",
    "        return (None, msg, board)\n",
    "    result = None\n",
    "    if board.is_checkmate():\n",
    "        msg = \"checkmate: \" + who(not board.turn) + \" wins!\"\n",
    "        result = not board.turn\n",
    "    elif board.is_stalemate():\n",
    "        msg = \"draw: stalemate\"\n",
    "    elif board.is_fivefold_repetition():\n",
    "        msg = \"draw: 5-fold repetition\"\n",
    "    elif board.is_insufficient_material():\n",
    "        msg = \"draw: insufficient material\"\n",
    "    elif board.can_claim_draw():\n",
    "        msg = \"draw: claim\"\n",
    "    if visual is not None:\n",
    "        print(msg)\n",
    "    return (result, msg, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play_game(human_player, random_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'chess' has no attribute 'polyglot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f48121e86622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzobrist_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'chess' has no attribute 'polyglot'"
     ]
    }
   ],
   "source": [
    "x = chess.polyglot.zobrist_hash(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from RLC.real_chess import agent, environment, learn, tree\n",
    "import chess\n",
    "from chess.pgn import Game\n",
    "\n",
    "opponent = agent.GreedyAgent()\n",
    "env = environment.Board(opponent, FEN=None)\n",
    "player = agent.Agent(lr=0.001, network='big')\n",
    "player.fix_model()\n",
    "learner = learn.TD_search(env, player, gamma=0.8, search_time=1.5)\n",
    "node = tree.Node(learner.env.board, gamma=learner.gamma)\n",
    "\n",
    "w_before = learner.agent.model.get_weights()\n",
    "n_iters = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.learn(iters=1, timelimit_seconds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.play_game(1,maxiter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(opponent.predict(np.expand_dims(env.layer_board, axis=0)))\n",
    "learner.search_time = 60\n",
    "learner.play_game(n_iters)\n",
    "pgn = Game.from_board(learner.env.board)\n",
    "with open(\"rlc_pgn\",\"w\") as log:\n",
    "    log.write(str(pgn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Idea :\n",
    "\n",
    "1. We have an interface to run our chess game\n",
    "2. The only thing we need now is an intelligent agent which we pass to the *player2* argument of function **play_game()** \n",
    "3. For this , I believe the policy DNN model stored in model.h5 file is crucial to building player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected keyword argument passed to optimizer: learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d3544230d90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#parameters to be passed to object of AIPlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RLC_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0moptimizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m       optimizer = optimizers.deserialize(\n\u001b[0;32m--> 249\u001b[0;31m           optimizer_config, custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m       \u001b[0;31m# Recover loss functions and metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m       printable_module_name='optimizer')\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    192\u001b[0m                 list(custom_objects.items())))\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lr, rho, epsilon, decay, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         raise TypeError('Unexpected keyword argument '\n\u001b[0;32m---> 68\u001b[0;31m                         'passed to optimizer: ' + str(k))\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0;31m# checks that clipnorm >= 0 and clipvalue >= 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected keyword argument passed to optimizer: learning_rate"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Flatten, Concatenate, Conv2D, Dropout\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import Model, clone_model, load_model\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import numpy as np\n",
    "\n",
    "gamma = 0.9\n",
    "search_time=60\n",
    "min_sim_count=10\n",
    "temperature=1\n",
    "\n",
    "#parameters to be passed to object of AIPlayer\n",
    "model = tf.keras.models.load_model('RLC_model.h5')\n",
    "env = environment.Board(opponent, FEN=None)\n",
    "node = tree.Node(learner.env.board, gamma=learner.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final class that represents the human vs AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " from RLC.real_chess.tree import Node\n",
    "    \n",
    "class AIPlayer(object):\n",
    "    \n",
    "    def __init__(self,env,model,tree):\n",
    "        self.env=env\n",
    "        self.gamma = 0.9\n",
    "        self.search_time=60\n",
    "        self.min_sim_count=10\n",
    "        self.temperature=1\n",
    "        self.model = model\n",
    "        self.tree = Node(self.env.board, gamma=learner.gamma)\n",
    "        self.k=1\n",
    "\n",
    "  \n",
    "    def mcts(self, node):\n",
    "        \"\"\"\n",
    "        Run Monte Carlo Tree Search\n",
    "        Args:\n",
    "            node: A game state node object\n",
    "\n",
    "        Returns:\n",
    "            the node with playout sims\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        starttime = time.time()\n",
    "        sim_count = 0\n",
    "        board_in = self.env.board.fen()\n",
    "\n",
    "        # First make a prediction for each child state\n",
    "        for move in self.env.board.generate_legal_moves():\n",
    "            if move not in node.children.keys():\n",
    "                node.children[move] = tree.Node(self.env.board, parent=node)\n",
    "\n",
    "            episode_end, reward = self.env.step(move)\n",
    "\n",
    "            if episode_end:\n",
    "                successor_state_value = 0\n",
    "            else:\n",
    "                successor_state_value = np.squeeze(\n",
    "                    self.model.predict(np.expand_dims(self.env.layer_board, axis=0))\n",
    "                )\n",
    "\n",
    "            child_value = reward + gamma * successor_state_value\n",
    "\n",
    "            node.update_child(move, child_value)\n",
    "            self.env.board.pop()\n",
    "            self.env.init_layer_board()\n",
    "        if not node.values:\n",
    "            node.values = [0]\n",
    "\n",
    "        while starttime + search_time > time.time() or sim_count < min_sim_count:\n",
    "            depth = 0\n",
    "            color = 1\n",
    "            node_rewards = []\n",
    "\n",
    "            # Select the best node from where to start MCTS\n",
    "            while node.children:\n",
    "                node, move = node.select(color=color)\n",
    "                if not move:\n",
    "                    # No move means that the node selects itself, not a child node.\n",
    "                    break\n",
    "                else:\n",
    "                    depth += 1\n",
    "                    color = color * -1  # switch color\n",
    "                    episode_end, reward = self.env.step(move)  # Update the environment to reflect the node\n",
    "                    node_rewards.append(reward)\n",
    "                    # Check best node is terminal\n",
    "\n",
    "                    if self.env.board.result() == \"1-0\" and depth == 1:  # -> Direct win for white, no need for mcts.\n",
    "                        self.env.board.pop()\n",
    "                        self.env.init_layer_board()\n",
    "                        node.update(1)\n",
    "                        node = node.parent\n",
    "                        return node\n",
    "                    elif episode_end:  # -> if the explored tree leads to a terminal state, simulate from root.\n",
    "                        while node.parent:\n",
    "                            self.env.board.pop()\n",
    "                            self.env.init_layer_board()\n",
    "                            node = node.parent\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            # Expand the game tree with a simulationd\n",
    "            Returns, move = node.simulate(self.model,\n",
    "                                            self.env,\n",
    "                                            temperature=temperature,\n",
    "                                            depth=0)\n",
    "            self.env.init_layer_board()\n",
    "\n",
    "            if move not in node.children.keys():\n",
    "                node.children[move] = tree.Node(self.env.board, parent=node)\n",
    "\n",
    "            node.update_child(move, Returns)\n",
    "\n",
    "            # Return to root node and backpropagate Returns\n",
    "            while node.parent:\n",
    "                latest_reward = node_rewards.pop(-1)\n",
    "                Returns = latest_reward + gamma * Returns\n",
    "                node.update(Returns)\n",
    "                node = node.parent\n",
    "\n",
    "                self.env.board.pop()\n",
    "                self.env.init_layer_board()\n",
    "            sim_count += 1\n",
    "\n",
    "        board_out = self.env.board.fen()\n",
    "        assert board_in == board_out\n",
    "\n",
    "        return node\n",
    "\n",
    "    def opponent_move(self):\n",
    "        start_mcts_after = -1\n",
    "        if self.k > start_mcts_after:\n",
    "            self.tree = self.mcts(self.tree)\n",
    "            # Step the best move\n",
    "            max_move = None\n",
    "            max_value = np.NINF\n",
    "            for move, child in self.tree.children.items():\n",
    "                sampled_value = np.mean(child.values)\n",
    "                if sampled_value > max_value:\n",
    "                    max_value = sampled_value\n",
    "                    max_move = move\n",
    "        else:\n",
    "            max_move = np.random.choice([move for move in self.env.board.generate_legal_moves()])\n",
    "\n",
    "        return max_move\n",
    "    \n",
    "    def greedy_opponent(self):\n",
    "        max_move = None\n",
    "        max_value = np.NINF\n",
    "        for move in self.env.board.generate_legal_moves():\n",
    "            self.env.step(move)\n",
    "            if self.env.board.result() == \"0-1\":\n",
    "                max_move = move\n",
    "                self.env.board.pop()\n",
    "                self.env.init_layer_board()\n",
    "                break\n",
    "            successor_state_value_opponent = self.env.opposing_agent.predict(\n",
    "                np.expand_dims(self.env.layer_board, axis=0))\n",
    "            if successor_state_value_opponent > max_value:\n",
    "                max_move = move\n",
    "                max_value = successor_state_value_opponent\n",
    "\n",
    "            self.env.board.pop()\n",
    "            self.env.init_layer_board()\n",
    "\n",
    "        return max_move\n",
    "    \n",
    "    def who(player):\n",
    "        return \"White\" if player == chess.WHITE else \"Black\"\n",
    "    \n",
    "    def display_board(board, use_svg):\n",
    "        if use_svg:\n",
    "            return board._repr_svg_()\n",
    "        else:\n",
    "            return \"<pre>\" + str(board) + \"</pre>\"\n",
    "\n",
    "    def play_game_ai(self,visual=\"svg\", pause=0.1):\n",
    "        \"\"\"\n",
    "        playerN1, player2: functions that takes board, return uci move\n",
    "        visual: \"simple\" | \"svg\" | None\n",
    "        \"\"\"\n",
    "        use_svg = (visual == \"svg\")\n",
    "        board_stop = display_board(self.env.board, use_svg)\n",
    "        html = \"%s\" % (board_stop)\n",
    "        display(HTML(html))\n",
    "        try:\n",
    "            while not self.env.board.is_game_over(claim_draw=True):\n",
    "                if self.env.board.turn != chess.WHITE:\n",
    "                    uci = self.human_player()\n",
    "                else:\n",
    "                    move = self.opponent_move()\n",
    "                    uci = move.uci()\n",
    "                name = who(self.env.board.turn)\n",
    "                self.env.board.push_uci(uci)\n",
    "                board_stop = display_board(self.env.board, use_svg)\n",
    "                html = \"<h1>Move %s %s, Play '%s':</h1><br/>%s\" % (\n",
    "                           len(self.env.board.move_stack), name, uci, board_stop)\n",
    "                if visual is not None:\n",
    "                    if visual == \"svg\":\n",
    "                        clear_output(wait=True)\n",
    "                    display(HTML(html))\n",
    "                    if visual == \"svg\":\n",
    "                        time.sleep(pause)\n",
    "        except KeyboardInterrupt:\n",
    "            msg = \"Game interrupted!\"\n",
    "            return (None, msg, self.env.board)\n",
    "        result = None\n",
    "        if self.env.board.is_checkmate():\n",
    "            msg = \"checkmate: \" + who(not self.env.board.turn) + \" wins!\"\n",
    "            result = not self.env.board.turn\n",
    "        elif self.env.board.is_stalemate():\n",
    "            msg = \"draw: stalemate\"\n",
    "        elif self.env.board.is_fivefold_repetition():\n",
    "            msg = \"draw: 5-fold repetition\"\n",
    "        elif self.env.board.is_insufficient_material():\n",
    "            msg = \"draw: insufficient material\"\n",
    "        elif self.env.board.can_claim_draw():\n",
    "            msg = \"draw: claim\"\n",
    "        if visual is not None:\n",
    "            print(msg)\n",
    "        #return (result, msg, self.env.board)\n",
    "        return (result,message)\n",
    "\n",
    "    def human_player(self):\n",
    "        st = input()\n",
    "        move = chess.Move.from_uci(st)\n",
    "        return move.uci()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_player = AIPlayer(env,model,node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play_game_ai(ai_player.opponent_move,human_player)\n",
    "env.board.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Move 1 White, Play 'e2e4':</h1><br/><svg height=\"400\" version=\"1.1\" viewBox=\"0 0 400 400\" width=\"400\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><g class=\"white pawn\" id=\"white-pawn\"><path d=\"M22 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38-1.95 1.12-3.28 3.21-3.28 5.62 0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-linecap=\"round\" stroke-width=\"1.5\" /></g><g class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-knight\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" style=\"fill:#000000; stroke:#000000;\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" /></g><g class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-bishop\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" id=\"white-rook\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" id=\"white-queen\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-king\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g class=\"black pawn\" id=\"black-pawn\"><path d=\"M22 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38-1.95 1.12-3.28 3.21-3.28 5.62 0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" stroke=\"#000\" stroke-linecap=\"round\" stroke-width=\"1.5\" /></g><g class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-knight\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" style=\"fill:#ececec; stroke:#ececec;\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-bishop\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" id=\"black-rook\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-linejoin=\"miter\" stroke-width=\"1\" /></g><g class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" id=\"black-queen\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-king\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect class=\"square dark a1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"335\" /><use transform=\"translate(20, 335)\" xlink:href=\"#white-rook\" /><rect class=\"square light b1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"335\" /><use transform=\"translate(65, 335)\" xlink:href=\"#white-knight\" /><rect class=\"square dark c1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"335\" /><use transform=\"translate(110, 335)\" xlink:href=\"#white-bishop\" /><rect class=\"square light d1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"335\" /><use transform=\"translate(155, 335)\" xlink:href=\"#white-queen\" /><rect class=\"square dark e1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"335\" /><use transform=\"translate(200, 335)\" xlink:href=\"#white-king\" /><rect class=\"square light f1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"335\" /><use transform=\"translate(245, 335)\" xlink:href=\"#white-bishop\" /><rect class=\"square dark g1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"335\" /><use transform=\"translate(290, 335)\" xlink:href=\"#white-knight\" /><rect class=\"square light h1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"335\" /><use transform=\"translate(335, 335)\" xlink:href=\"#white-rook\" /><rect class=\"square light a2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"290\" /><use transform=\"translate(20, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark b2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"290\" /><use transform=\"translate(65, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square light c2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"290\" /><use transform=\"translate(110, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark d2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"290\" /><use transform=\"translate(155, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square light lastmove e2\" fill=\"#cdd16a\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"290\" /><rect class=\"square dark f2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"290\" /><use transform=\"translate(245, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square light g2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"290\" /><use transform=\"translate(290, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark h2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"290\" /><use transform=\"translate(335, 290)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark a3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"245\" /><rect class=\"square light b3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"245\" /><rect class=\"square dark c3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"245\" /><rect class=\"square light d3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"245\" /><rect class=\"square dark e3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"245\" /><rect class=\"square light f3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"245\" /><rect class=\"square dark g3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"245\" /><rect class=\"square light h3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"245\" /><rect class=\"square light a4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"200\" /><rect class=\"square dark b4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"200\" /><rect class=\"square light c4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"200\" /><rect class=\"square dark d4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"200\" /><rect class=\"square light lastmove e4\" fill=\"#cdd16a\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"200\" /><use transform=\"translate(200, 200)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark f4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"200\" /><rect class=\"square light g4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"200\" /><rect class=\"square dark h4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"200\" /><rect class=\"square dark a5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"155\" /><rect class=\"square light b5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"155\" /><rect class=\"square dark c5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"155\" /><rect class=\"square light d5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"155\" /><rect class=\"square dark e5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"155\" /><rect class=\"square light f5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"155\" /><rect class=\"square dark g5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"155\" /><rect class=\"square light h5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"155\" /><rect class=\"square light a6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"110\" /><rect class=\"square dark b6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"110\" /><rect class=\"square light c6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"110\" /><rect class=\"square dark d6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"110\" /><rect class=\"square light e6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"110\" /><rect class=\"square dark f6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"110\" /><rect class=\"square light g6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"110\" /><rect class=\"square dark h6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"110\" /><rect class=\"square dark a7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"65\" /><use transform=\"translate(20, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light b7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"65\" /><use transform=\"translate(65, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark c7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"65\" /><use transform=\"translate(110, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light d7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"65\" /><use transform=\"translate(155, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark e7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"65\" /><use transform=\"translate(200, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light f7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"65\" /><use transform=\"translate(245, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark g7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"65\" /><use transform=\"translate(290, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light h7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"65\" /><use transform=\"translate(335, 65)\" xlink:href=\"#black-pawn\" /><rect class=\"square light a8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"20\" y=\"20\" /><use transform=\"translate(20, 20)\" xlink:href=\"#black-rook\" /><rect class=\"square dark b8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"65\" y=\"20\" /><use transform=\"translate(65, 20)\" xlink:href=\"#black-knight\" /><rect class=\"square light c8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"110\" y=\"20\" /><use transform=\"translate(110, 20)\" xlink:href=\"#black-bishop\" /><rect class=\"square dark d8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"155\" y=\"20\" /><use transform=\"translate(155, 20)\" xlink:href=\"#black-queen\" /><rect class=\"square light e8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"200\" y=\"20\" /><use transform=\"translate(200, 20)\" xlink:href=\"#black-king\" /><rect class=\"square dark f8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"245\" y=\"20\" /><use transform=\"translate(245, 20)\" xlink:href=\"#black-bishop\" /><rect class=\"square light g8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"290\" y=\"20\" /><use transform=\"translate(290, 20)\" xlink:href=\"#black-knight\" /><rect class=\"square dark h8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"335\" y=\"20\" /><use transform=\"translate(335, 20)\" xlink:href=\"#black-rook\" /><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"42\" y=\"10\">a</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"42\" y=\"390\">a</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"87\" y=\"10\">b</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"87\" y=\"390\">b</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"132\" y=\"10\">c</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"132\" y=\"390\">c</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"177\" y=\"10\">d</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"177\" y=\"390\">d</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"222\" y=\"10\">e</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"222\" y=\"390\">e</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"267\" y=\"10\">f</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"267\" y=\"390\">f</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"312\" y=\"10\">g</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"312\" y=\"390\">g</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"357\" y=\"10\">h</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"357\" y=\"390\">h</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"357\">1</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"357\">1</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"312\">2</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"312\">2</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"267\">3</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"267\">3</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"222\">4</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"222\">4</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"177\">5</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"177\">5</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"132\">6</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"132\">6</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"87\">7</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"87\">7</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"10\" y=\"42\">8</text><text alignment-baseline=\"middle\" font-size=\"14\" text-anchor=\"middle\" x=\"390\" y=\"42\">8</text></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-afe3564b0e5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#uncomment and run below code to play aginst greedy opponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mai_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game_ai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-3ac4e4e00045>\u001b[0m in \u001b[0;36mplay_game_ai\u001b[0;34m(self, visual, pause)\u001b[0m\n\u001b[1;32m    171\u001b[0m            \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_game_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim_draw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                    \u001b[0muci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                    \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopponent_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-3ac4e4e00045>\u001b[0m in \u001b[0;36mhuman_player\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mhuman_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m        \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m        \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_uci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             raise StdinNotImplementedError(\n\u001b[0;32m--> 855\u001b[0;31m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             )\n\u001b[1;32m    857\u001b[0m         return self._input_request(str(prompt),\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "#uncomment and run below code to play aginst greedy opponent\n",
    "\n",
    "#ai_player.play_game_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.board.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from RLC.real_chess.tree import Node\n",
    "import math\n",
    "import gc\n",
    "\n",
    "\n",
    "def softmax(x, temperature=1):\n",
    "    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "class TD_search_m(object):\n",
    "\n",
    "    def __init__(self, env, agent, gamma=0.9, search_time=1, memsize=2000, batch_size=256, temperature=1):\n",
    "        \"\"\"\n",
    "        Chess algorithm that combines bootstrapped monte carlo tree search with Q Learning\n",
    "        Args:\n",
    "            env: RLC chess environment\n",
    "            agent: RLC chess agent\n",
    "            gamma: discount factor\n",
    "            search_time: maximum time spent doing tree search\n",
    "            memsize: Amount of training samples to keep in-memory\n",
    "            batch_size: Size of the training batches\n",
    "            temperature: softmax temperature for mcts\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.tree = Node(self.env)\n",
    "        self.gamma = gamma\n",
    "        self.memsize = memsize\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.reward_trace = []  # Keeps track of the rewards\n",
    "        self.piece_balance_trace = []  # Keep track of the material value on the board\n",
    "        self.ready = False  # Whether to start training\n",
    "        self.search_time = search_time\n",
    "        self.min_sim_count = 10\n",
    "\n",
    "        self.mem_state = np.zeros(shape=(1, 8, 8, 8))\n",
    "        self.mem_sucstate = np.zeros(shape=(1, 8, 8, 8))\n",
    "        self.mem_reward = np.zeros(shape=(1))\n",
    "        self.mem_error = np.zeros(shape=(1))\n",
    "        self.mem_episode_active = np.ones(shape=(1))\n",
    "\n",
    "    #def display_board(self):\n",
    "     #   return \"<pre>\" + str(self.env.board) + \"</pre>\"\n",
    "        \n",
    "    def play_game(self, k, maxiter=80):\n",
    "        \"\"\"\n",
    "        Play a chess game and learn from it\n",
    "        Args:\n",
    "            k: the play iteration number\n",
    "            maxiter: maximum duration of the game (halfmoves)\n",
    "\n",
    "        Returns:\n",
    "            board: Chess environment on terminal state\n",
    "        \"\"\"\n",
    "        episode_end = False\n",
    "        turncount = 0\n",
    "        tree = Node(self.env.board, gamma=self.gamma)  # Initialize the game tree\n",
    "\n",
    "        # Play a game of chess\n",
    "        # According to test.py - Decides the best max_move and max_value using MCTS\n",
    "        while not episode_end:\n",
    "            state = np.expand_dims(self.env.layer_board.copy(), axis=0)\n",
    "            state_value = self.agent.predict(state)\n",
    "\n",
    "            board_stop = display_board(self.env.board,\"svg\")\n",
    "            html = \"%s\" % (board_stop)\n",
    "            display(HTML(html))\n",
    "            # White's turn involves tree-search\n",
    "            if self.env.board.turn:\n",
    "\n",
    "                # Do a Monte Carlo Tree Search after game iteration k\n",
    "                start_mcts_after = -1\n",
    "                if k > start_mcts_after:\n",
    "                    tree = self.mcts(tree)\n",
    "                    # Step the best move\n",
    "                    max_move = None\n",
    "                    max_value = np.NINF\n",
    "                    for move, child in tree.children.items():\n",
    "                        sampled_value = np.mean(child.values)\n",
    "                        if sampled_value > max_value:\n",
    "                            max_value = sampled_value\n",
    "                            max_move = move\n",
    "                else:\n",
    "                    max_move = np.random.choice([move for move in self.env.board.generate_legal_moves()])\n",
    "\n",
    "            # Black's turn is myopic\n",
    "            # According to test.py - uses greedy approach to decide the\n",
    "            # best max_move, and its corresponding max_value\n",
    "            else:\n",
    "                max_move = None\n",
    "                max_value = np.NINF\n",
    "                \n",
    "                st = input()\n",
    "                move = chess.Move.from_uci(st)\n",
    "                max_move = move\n",
    "                \n",
    "            if not (self.env.board.turn and max_move not in tree.children.keys()) or not k > start_mcts_after:\n",
    "                tree.children[max_move] = Node(gamma=0.9, parent=tree)\n",
    "\n",
    "            episode_end, reward = self.env.step(max_move)\n",
    "\n",
    "            tree = tree.children[max_move]\n",
    "            tree.parent = None\n",
    "            gc.collect()\n",
    "\n",
    "            sucstate = np.expand_dims(self.env.layer_board, axis=0)\n",
    "            new_state_value = self.agent.predict(sucstate)\n",
    "\n",
    "            error = reward + self.gamma * new_state_value - state_value\n",
    "            error = np.float(np.squeeze(error))\n",
    "\n",
    "            turncount += 1\n",
    "            if turncount > maxiter and not episode_end:\n",
    "                episode_end = True\n",
    "\n",
    "            episode_active = 0 if episode_end else 1\n",
    "\n",
    "            # construct training sample state, prediction, error\n",
    "            self.mem_state = np.append(self.mem_state, state, axis=0)\n",
    "            self.mem_reward = np.append(self.mem_reward, reward)\n",
    "            self.mem_sucstate = np.append(self.mem_sucstate, sucstate, axis=0)\n",
    "            self.mem_error = np.append(self.mem_error, error)\n",
    "            self.reward_trace = np.append(self.reward_trace, reward)\n",
    "            self.mem_episode_active = np.append(self.mem_episode_active, episode_active)\n",
    "\n",
    "            if self.mem_state.shape[0] > self.memsize:\n",
    "                self.mem_state = self.mem_state[1:]\n",
    "                self.mem_reward = self.mem_reward[1:]\n",
    "                self.mem_sucstate = self.mem_sucstate[1:]\n",
    "                self.mem_error = self.mem_error[1:]\n",
    "                self.mem_episode_active = self.mem_episode_active[1:]\n",
    "                gc.collect()\n",
    "\n",
    "            if turncount % 10 == 0:\n",
    "                self.update_agent()\n",
    "\n",
    "        piece_balance = self.env.get_material_value()\n",
    "        self.piece_balance_trace.append(piece_balance)\n",
    "        print(\"game ended with result\", reward, \"and material balance\", piece_balance, \"in\", turncount, \"halfmoves\")\n",
    "\n",
    "        return self.env.board\n",
    "\n",
    "    def update_agent(self):\n",
    "        \"\"\"\n",
    "        Update the Agent with TD learning\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.ready:\n",
    "            choice_indices, states, rewards, sucstates, episode_active = self.get_minibatch()\n",
    "            td_errors = self.agent.TD_update(states, rewards, sucstates, episode_active, gamma=self.gamma)\n",
    "            self.mem_error[choice_indices.tolist()] = td_errors\n",
    "\n",
    "    def get_minibatch(self, prioritized=True):\n",
    "        \"\"\"\n",
    "        Get a mini batch of experience\n",
    "        Args:\n",
    "            prioritized:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        if prioritized:\n",
    "            sampling_priorities = np.abs(self.mem_error) + 1e-9\n",
    "        else:\n",
    "            sampling_priorities = np.ones(shape=self.mem_error.shape)\n",
    "        sampling_probs = sampling_priorities / np.sum(sampling_priorities)\n",
    "        sample_indices = [x for x in range(self.mem_state.shape[0])]\n",
    "        choice_indices = np.random.choice(sample_indices,\n",
    "                                          min(self.mem_state.shape[0],\n",
    "                                              self.batch_size),\n",
    "                                          p=np.squeeze(sampling_probs),\n",
    "                                          replace=False\n",
    "                                          )\n",
    "        states = self.mem_state[choice_indices]\n",
    "        rewards = self.mem_reward[choice_indices]\n",
    "        sucstates = self.mem_sucstate[choice_indices]\n",
    "        episode_active = self.mem_episode_active[choice_indices]\n",
    "\n",
    "        return choice_indices, states, rewards, sucstates, episode_active\n",
    "\n",
    "    def mcts(self, node):\n",
    "        \"\"\"\n",
    "        Run Monte Carlo Tree Search\n",
    "        Args:\n",
    "            node: A game state node object\n",
    "\n",
    "        Returns:\n",
    "            the node with playout sims\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        starttime = time.time()\n",
    "        sim_count = 0\n",
    "        board_in = self.env.board.fen()\n",
    "\n",
    "        # First make a prediction for each child state\n",
    "        for move in self.env.board.generate_legal_moves():\n",
    "            if move not in node.children.keys():\n",
    "                node.children[move] = Node(self.env.board, parent=node)\n",
    "\n",
    "            episode_end, reward = self.env.step(move)\n",
    "\n",
    "            if episode_end:\n",
    "                successor_state_value = 0\n",
    "            else:\n",
    "                successor_state_value = np.squeeze(\n",
    "                    self.agent.model.predict(np.expand_dims(self.env.layer_board, axis=0))\n",
    "                )\n",
    "\n",
    "            child_value = reward + self.gamma * successor_state_value\n",
    "\n",
    "            node.update_child(move, child_value)\n",
    "            self.env.board.pop()\n",
    "            self.env.init_layer_board()\n",
    "        if not node.values:\n",
    "            node.values = [0]\n",
    "\n",
    "        while starttime + self.search_time > time.time() or sim_count < self.min_sim_count:\n",
    "            depth = 0\n",
    "            color = 1\n",
    "            node_rewards = []\n",
    "\n",
    "            # Select the best node from where to start MCTS\n",
    "            while node.children:\n",
    "                node, move = node.select(color=color)\n",
    "                if not move:\n",
    "                    # No move means that the node selects itself, not a child node.\n",
    "                    break\n",
    "                else:\n",
    "                    depth += 1\n",
    "                    color = color * -1  # switch color\n",
    "                    episode_end, reward = self.env.step(move)  # Update the environment to reflect the node\n",
    "                    node_rewards.append(reward)\n",
    "                    # Check best node is terminal\n",
    "\n",
    "                    if self.env.board.result() == \"1-0\" and depth == 1:  # -> Direct win for white, no need for mcts.\n",
    "                        self.env.board.pop()\n",
    "                        self.env.init_layer_board()\n",
    "                        node.update(1)\n",
    "                        node = node.parent\n",
    "                        return node\n",
    "                    elif episode_end:  # -> if the explored tree leads to a terminal state, simulate from root.\n",
    "                        while node.parent:\n",
    "                            self.env.board.pop()\n",
    "                            self.env.init_layer_board()\n",
    "                            node = node.parent\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            # Expand the game tree with a simulation\n",
    "            Returns, move = node.simulate(self.agent.fixed_model,\n",
    "                                          self.env,\n",
    "                                          temperature=self.temperature,\n",
    "                                          depth=0)\n",
    "            self.env.init_layer_board()\n",
    "\n",
    "            if move not in node.children.keys():\n",
    "                node.children[move] = Node(self.env.board, parent=node)\n",
    "\n",
    "            node.update_child(move, Returns)\n",
    "\n",
    "            # Return to root node and backpropagate Returns\n",
    "            while node.parent:\n",
    "                latest_reward = node_rewards.pop(-1)\n",
    "                Returns = latest_reward + self.gamma * Returns\n",
    "                node.update(Returns)\n",
    "                node = node.parent\n",
    "\n",
    "                self.env.board.pop()\n",
    "                self.env.init_layer_board()\n",
    "            sim_count += 1\n",
    "\n",
    "        board_out = self.env.board.fen()\n",
    "        assert board_in == board_out\n",
    "\n",
    "        return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = TD_search_m(env, player, gamma=0.8, search_time=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.play_game(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
